<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Stay hungry. Stay foolish </title>
    <link>https://sunprinces.github.io/learning/topics/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning on Stay hungry. Stay foolish </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017. All rights reserved.</copyright>
    <lastBuildDate>Thu, 10 Jan 2019 11:59:24 +0800</lastBuildDate>
    
	<atom:link href="https://sunprinces.github.io/learning/topics/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reinforcement Learning - Lec6</title>
      <link>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec6/</link>
      <pubDate>Thu, 10 Jan 2019 11:59:24 +0800</pubDate>
      
      <guid>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec6/</guid>
      <description>&lt;p&gt;在這一講中，要介紹的是 RL 中的 supervised 系方法 - Imitation Learning。想法是收集 expert (or say, &amp;ldquo;ground-truth&amp;rdquo; agent) 與 environment 互動的&lt;span&gt;$(s,a)$&lt;/span&gt; pairs 去 train 我們的 agent/actor。那麼該如何利用這些 &lt;span&gt;$(s,a)$&lt;/span&gt; pair 呢？&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reinforcement Learning - Lec5</title>
      <link>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec5/</link>
      <pubDate>Tue, 08 Jan 2019 10:59:24 +0800</pubDate>
      
      <guid>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec5/</guid>
      <description>&lt;p&gt;Lec1 - Lec4  分別介紹了 Policy-based 及 Value-based 的 RL algorithm ，而這一講要
介紹的 Actor Critic 則是同時用到了兩個演算法的部份，並在 biased 與否 (準不準) 及 variance
高低 (好不好 train) 提供一個可以調控的 hyperparameter 讓我們選擇。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reinforcement Learning - Lec4</title>
      <link>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec4/</link>
      <pubDate>Sun, 06 Jan 2019 15:59:24 +0800</pubDate>
      
      <guid>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec4/</guid>
      <description>&lt;p&gt;在這講中，要討論的是 non-tabular case 的問題 (就是上一講中無法保證收斂的那些QQ)，我們選定 Neural Network 作為拿來 approximate &lt;span&gt;$Q(s,a)$&lt;/span&gt; 的 function family，且不是用一般 regression 的方法找 &lt;span&gt;$\phi$&lt;/span&gt; (畢竟它的 target &lt;span&gt;$y$&lt;/span&gt; 也只是中間產物，並非 optimal Q)，而是 N-step 的 gradient descent。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reinforcement Learning - Lec3</title>
      <link>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec3/</link>
      <pubDate>Sat, 05 Jan 2019 08:59:24 +0800</pubDate>
      
      <guid>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec3/</guid>
      <description>&lt;p&gt;前兩講 focus 在 policy-based 的 RL 演算法，直接 learn 一組參數去 parametrize
policy。而後續兩講則會 focus 在 value-based 的方法，想法是算出 &lt;span&gt;$V^\pi(s), Q^\pi(s,a)$&lt;/span&gt; (同樣可以 approximately parametrized by &lt;span&gt;$\theta$&lt;/span&gt;)，而所對應的 policy 則是去選擇 Given &lt;span&gt;$s$&lt;/span&gt;，好度最高的 action &lt;span&gt;$a$&lt;/span&gt; (w/ appropriate exploration)。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reinforcement Learning - Lec2</title>
      <link>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec2/</link>
      <pubDate>Thu, 03 Jan 2019 10:59:24 +0800</pubDate>
      
      <guid>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec2/</guid>
      <description>&lt;p&gt;上回提到了 policy gradint 的方法，及其缺點，這一講會介紹各種改進的方法。包括降低
sample 的 variance 及 off-policy (使得 data 更有效地被利用)。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reinforcement Learning - Lec1</title>
      <link>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec1/</link>
      <pubDate>Tue, 01 Jan 2019 11:59:24 +0800</pubDate>
      
      <guid>https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec1/</guid>
      <description>&lt;p&gt;今年1 月的目標想複習三下時學的 RL，主要的參考教材為李宏毅老師的 DRL 8 講及 Sergey 在
Berkeley 開的 CS294。&lt;/p&gt;
&lt;p&gt;先讓我們從 Policy Gradient 開始吧！&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>