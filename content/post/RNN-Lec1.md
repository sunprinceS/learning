+++
date =  "2018-05-20T16:45:08+01:00"
description = "Neural Turing Machine (NTM)"
tags = ["Machine Learning","RNN"]
title =  "Beyond RNN - Lec1"
topics = ["Beyond RNN"]
draft = true
+++

é€™å€‹ç³»åˆ—æœƒ focus åœ¨è¿‘å¹´ä¾†å„å¼å„æ¨£æˆ‘è¦ºå¾—æœ‰è¶£çš„ RNN è®Šå½¢ï¼Œä»¥åŠç›¸é—œçš„å¯¦åšã€‚

<!--more-->

æœ€è¿‘å‰›å¥½åœ¨å­¸ç¿’ PyTorchï¼Œä¾¿æƒ³æ‰¾å€‹é©åˆçš„é¡Œç›®ä¾†ç·´æ‰‹ï¼Œä½†ä¸€æ˜§åœ° implement å’Œå¯«æ‰£æœ‰äº›ä¹å‘³ï¼Œå¸Œæœ›è‡ªå·±èƒ½åŒæ™‚éå›º programming åˆå­¸ç¿’æ–°çš„å­¸ç†çŸ¥è­˜ï¼Œä¹Ÿå› æ­¤æœ‰äº†é€™å€‹ç³»åˆ— - Beyond RNNã€‚

æœ‰æ¥è§¸é DL çš„è®€è€…ï¼Œæƒ³å¿…å° RNN (Recurrent Neural Network) ä¸¦ä¸é™Œç”Ÿï¼Œå®ƒä¸»è¦æ‡‰ç”¨åœ¨è³‡æ–™æœ‰é †åºæ€§çš„ scenerio (e.g èªéŸ³,æ–‡å­—,è‚¡ç¥¨èµ°å‹¢(?ğŸ˜›) ...)ï¼Œé™¤äº† single cell çš„è®Šå½¢å¦‚ LSTM (Long Short Term Memory) å’Œ GRU (Gated Recurrent Unit)ä¹‹å¤–ï¼Œå¦å¤–æœ‰è¨±å¤šå°‡å…¶ equip åœ¨ä¸åŒçš„æ¶æ§‹ä¸Šï¼Œå¼·åŒ–å…¶åŠŸèƒ½çš„è¨±å¤šå»¶ä¼¸ã€‚

## Introduction

ä»Šå¤©æ‰€è¦å¸¶ä¾†çš„æ˜¯ [Neural Turing Machine](https://arxiv.org/abs/1410.5401) ï¼Œæ˜¯ DeepMind åœ¨ 2014 å¹´
æ‰€ç™¼è¡¨çš„ç ”ç©¶ã€‚

RNN-type çš„ network ï¼Œä¹‹æ‰€ä»¥èƒ½è™•ç† sequence çš„å•é¡Œï¼Œé—œéµåœ¨å®ƒ keep äº† hidden
state (å°æ‡‰åˆ° LSTM çš„ <span>$h,c$</span>... )ï¼Œå¯ä»¥ç†è§£æˆç•¶ä½œåˆ°ç›®å‰ç‚ºæ­¢ï¼Œ**æ‰€æœ‰ input çš„
representation** ï¼Œè€Œå…¶åˆæœƒéš¨è‘—æ–°çš„ input é€²ä¾†è€Œæœ‰æ‰€æ”¹è®Šã€‚

## Motivation

ä¸€å€‹å¾ˆè‡ªç„¶çš„å•é¡Œæ˜¯ï¼Œå°æ–¼å¾ˆé•·çš„é‚£äº› sequence ï¼Œæˆ‘å€‘éœ€è¦é–‹å¤šå¤§çš„ hidden state å»
maintainï¼Ÿ å¾ˆç›´è¦ºçš„æ‡‰è©²æ˜¯ dimension è¦æ›´å¤§ï¼Œå¯ä»¥æƒ³æˆæˆ‘å€‘è¦ç”¨ä¸€æ¢å¹¾åƒç¶­çš„ vector ä¾†ç•¶æˆæˆ‘å€‘çš„ hidden stateã€‚

å°±åƒ **CNN** çš„ motivation ä¸€æ¨£ï¼Œæˆ‘å€‘å¸Œæœ›æˆ‘å€‘çš„ model èƒ½å¼•é€²å¤šä¸€é»å…ˆå‚™çš„è³‡è¨Šï¼Œå°æ–¼å½±åƒï¼Œç›´æ¥æŠŠæ¯å€‹ pixel å±•å¹³æˆ 1D vectorï¼Œå…¶ performance æ¯”ä¸ä¸Šå°‡å…¶ treat æˆ 2D Tensor ï¼Œè€ƒæ…®é„°è¿‘ pixel çš„ é¡å¤– informationã€‚(å·¥å•†æ™‚é–“: é€™è£¡æœ‰æˆ‘ä¹‹å‰ç•¶ TA æ™‚å‡ºçµ¦åŒå­¸çš„[ç·´ç¿’](https://sunprinces.github.io/ML-Assignment3/)ï¼Œåœ¨ FER é€™å€‹ dataset ä¸Šæ¯”è¼ƒ CNN è·Ÿ DNN çš„ performance å„ªåŠ£)

è€Œ NTM çš„ idea åœ¨æ–¼å¼•é€² extra çš„ memory ï¼Œä¹Ÿå¯ä»¥ç†è§£æˆæŠŠåŸå…ˆé‚£å€‹ä¸Šåƒç¶­çš„ 1D
hidden vectorï¼ŒæŠ˜æˆ 2D Tensor ä½œç‚º memoryï¼Œä¸¦ä»¿æ•ˆç¾åœ¨é›»è…¦çš„æ¶æ§‹ï¼Œæ§‹é€ å…¶èˆ‡ input
interact çš„ mechanismã€‚

## Basic Structure
<center><img src="/img/post/ntm-structure.png" width="50%" style="border-radius: 0%;"></center>

è®€å¯« memory é€™ä»¶äº‹åˆ†æˆå…©å€‹æ­¥é©Ÿï¼Œå…§å®¹åŠä½ç½®ã€‚ä»”ç´°æƒ³æƒ³ï¼Œã€Œä½ç½®ã€åœ¨ memory ä¸­ï¼Œæ‡‰è©²
æ˜¯é›¢æ•£çš„å­˜åœ¨ï¼Œè©²å¦‚ä½•æŠŠå¾å“ªè£¡è®€/å¯«é€™ä»¶äº‹è®Šæˆå¯ä»¥ç”¨ NN å¯ä»¥ train å‘¢ï¼Ÿæƒ³æ³•å°±æ˜¯è®“å…¶
**å¯å¾®**ï¼Œåœ¨é€™è£¡çš„ä½œæ³•æ˜¯æ¯æ¬¡åœ¨è®€å¯«æ™‚ï¼Œä¸æ˜¯å° specific ä½ç½®çš„ memory cell åšæ“ä½œï¼Œè€Œ
æ˜¯å°ä¸€å€‹ specific distribution <span>$w$</span> çš„ memory cell åšæ“ä½œ (ç°¡å–®ä¾†èªªï¼Œå°±æ˜¯åŒæ™‚è€ƒæ…®æ‰€æœ‰
çš„ memory cellï¼Œä½†æ ¹æ“šè£¡é ­å…§å®¹çš„ä¸åŒï¼Œæ±ºå®šå…¶æ‰€ä½”çš„ weight)ã€‚

## Read & Write
å‡è¨­æ™‚é–“é» <span>$t$</span> ï¼Œæˆ‘å€‘æœ‰äº†ä¸€å€‹ address distribution <span>$w_t$</span>ï¼Œè®€å‡ºä¾†çš„å…§å®¹å¾ˆç°¡å–®å°±æ˜¯

<div>
\[
\mathbf{r}_t \leftarrow \sum_i w_t(i)\mathbf{M}(i)
\]
</div>

```python
 def read(self, w):
        """ Read memory corresponding to the address weighting
        Arguments:
            w: shape = (batch_size,N)
        Outputs:
            shape = (batch_size,M)
        """
        return torch.matmul(w.unsqueeze(1), self.memory).squeeze(1)
```

å¯«å…¥ memory å°±æ˜¯

<div>
\[
\mathbf{M}_t(i) \leftarrow \tilde \mathbf{M}_t(i)[\mathbf{1} - w_t(i) \mathbf{e}_t] + w_t(i)\mathbf{a}_t
\]
</div>

```python
def write(self, w, e, a):
        """ Erase/Add memory corresponding to the address weighting
        Arguments:
            w: shape = (batch_size,N)
            e,a: shape = (batch_size,M)
        """
        self.prev_mem = self.memory
        erase = torch.matmul(w.unsqueeze(-1), e.unsqueeze(1))
        add = torch.matmul(w.unsqueeze(-1), a.unsqueeze(1))
        self.memory = self.prev_mem * (1 - erase) + add
```

## Address Mechanism

æˆ‘è¦ºå¾—é€™éƒ¨ä»½å°±æ˜¯ NTM ä¸­æœ€ç²¾è¯çš„ä¸€æ®µäº†ï¼Œè¨è«–äº†å¦‚ä½•å¾—åˆ° address distribution
<span>$w_t$</span>ï¼Œæ‰€ç”¨åˆ°çš„æ¦‚å¿µï¼Œä¹Ÿæ˜¯ä¹‹å¾Œ attention-based model çš„ motivationã€‚

### Content-based Attention

æ ¹æ“š memory cell ç¾åœ¨æ‰€ç´€éŒ„çš„å…§å®¹èˆ‡ controller çš„ output key vector <span>$\mathbf{k}$</span> (äº¦å³ <span>$f$</span>(input))åšæ¯”è¼ƒï¼Œæ±ºå®šè©² memory cell æ‰€è¦ä½”çš„æ¯”é‡ã€‚

<div>
\[
w_c(i) \leftarrow \frac{e^{\beta K[\textbf{k},\textbf{M}(i)] } }{ \sum_{j} e^{ \beta K[\textbf{k},\textbf{M}(j)] } }
\]
</div>

**Note:** <span>$K[u,v]$</span>æ˜¯ similarity çš„ measureï¼Œä¸€èˆ¬ç”¨ cosine
similarity

### Location-based Attention

As titleï¼Œæˆ‘å€‘å¾ä¸Šä¸€æ­¥ä¸­ï¼Œå¾—åˆ°äº† <span>$w_c$</span>ï¼Œä½†æ ¹æ“šä¸åŒçš„ input ï¼Œåˆå†å¤š
è€ƒæ…®é€™äº› moemory cell èˆ‡å…¶é„°è¿‘è€…çš„ distribution ã€‚

#### Interpolation

è€ƒæ…®ä¸Šä¸€å€‹æ™‚é–“é» address weighting çš„å½±éŸ¿

<div>
\[
\textbf{w}_{t} \leftarrow g \textbf{w}_{t} + (1-g) \textbf{w}_{t-1}
\]
</div>

#### Convolution Shift
å°‡æŸä¸€ä½ç½®çš„ memory cell ä¹‹ weighting ä¸Šï¼Œå†è€ƒæ…®å…¶èˆ‡é„°è¿‘ memory cell çš„
distribution (åœ¨æ­¤æˆ‘å€‘è€ƒæ…®å·¦å³å„ä¸€å€‹ memoryï¼Œå¦‚æœ weight æ˜¯ <span>$(0,0,1)$</span> çš„è©±ï¼Œæ„æ€
å°±æ˜¯åŸå…ˆè©² cell å¾—åˆ°çš„æ‰€æœ‰ weight éƒ½ apply çµ¦å…¶å³é‚Šçš„ cell)

<div>
\[
w(i) \leftarrow w(i+1) s(-1) + w(i)s(0) + w(i-1)s(1)
\]
</div>

#### Reweighting - Sharpening
<div>
\[
w(i) \leftarrow \frac{w(i)^{\gamma}}{\sum_{j}w(j)^{\gamma}}
\]
</div>


ç¶œåˆèµ·ä¾†ï¼Œæ•´å€‹ mechanism æ˜¯é€™æ¨£é‹ä½œçš„ã€‚
<center><img src="/img/post/ntm-address-mechanism.png" width="80%" style="border-radius: 0%;"></center>

```python
def address(self, k, beta, g, s, gamma, w_prev):
       # Content focus
       wc = self._similarity(k, beta)

       # Location focus
       wg = self._interpolate(w_prev, wc, g)
       w_hat = self._shift(wg, s)
       w = self._sharpen(w_hat, gamma)

       return w

   def _similarity(self, k, beta):
       cos_sim = F.cosine_similarity(self.memory + 1e-16, k.unsqueeze(1) + 1e-16, dim=-1)
       w = F.softmax(beta * cos_sim , dim=1)
       return w

   def _interpolate(self, w_prev, wc, g):
       return g * wc + (1 - g) * w_prev

   def _shift(self, wg, s):
       #consider 3 locations together
       conved = torch.cat([wg[:,-1:],wg,wg[:,:1]],dim=1) #pad in the beginning and end
       result = F.conv1d(conved.unsqueeze(1),s.unsqueeze(1))
       return torch.cat([result[i:i+1,i,:] for i in range(self.batch_size)])

   def _sharpen(self, w_hat, gamma):
       w = w_hat ** gamma
       w = torch.div(w, torch.sum(w, dim=1).view(-1, 1) + 1e-16)
       return w
```

## Discussion & Conclusion
æˆ‘åƒè€ƒäº†åˆ¥äººçš„ implementationï¼Œ reorganize äº†è‡ªå·±çš„ [NTM](https://github.com/sunprinceS/Neural-Turing-Machine)ï¼Œè€Œåœ¨ Tensorboardï¼ŒåŠ é€²äº†æŸäº› task çš„ visualizationã€‚

## Reference

* [Distill ä¸Šçš„æ–‡ç« ](https://distill.pub/2016/augmented-rnns/)
* [åŸå§‹è«–æ–‡](https://arxiv.org/abs/1410.5401)
