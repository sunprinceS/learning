+++
date = "2019-01-08T10:59:24+08:00"
description = "Improvements of naive REINFORCE algorithm"
tags = ["RL","NTU","CS294"]
title =  "Reinforcement Learning - Lec2"
topics = ["Reinforcement Learning"]
+++

ä¸Šå›æåˆ°äº† policy gradint çš„æ–¹æ³•ï¼ŒåŠå…¶ç¼ºé»ï¼Œé€™ä¸€è¬›æœƒä»‹ç´¹å„ç¨®æ”¹é€²çš„æ–¹æ³•ã€‚åŒ…æ‹¬é™ä½
sample çš„ variance åŠ off-policy (ä½¿å¾— data æ›´æœ‰æ•ˆåœ°è¢«åˆ©ç”¨)ã€‚

<!--more-->

## Reduce Variance

### Baseline 
åŸå…ˆçš„ gradient æ˜¯ä»¥ <span>$r(\tau)$</span> ä½œç‚º weight ï¼Œç¾åœ¨å‰‡æ˜¯æ¸›å»ä¸€å€‹å¸¸æ•¸
<span>$b \approx \mathbb{E}[r(\tau)]$</span>ã€‚
æƒ³æ³•æ˜¯å¯èƒ½æ¯å€‹ <span>$\tau$</span>æ‰€æ”¶åˆ°çš„ reward éƒ½æ˜¯æ­£çš„ï¼Œåœ¨ä¸‹ä¸€æ¬¡æ›´æ–°å¾Œï¼Œ
policy æ›´æœƒ prefer å»é¸æ“‡é€™äº› <span>$(s,a)$</span> ï¼Œä½†å› ç‚ºç•¢ç«Ÿæˆ‘å€‘æ˜¯ç”¨ sample çš„æ–¹å¼ï¼Œå°šæœ‰è¨±å¤š <span>$(s,a)$</span> æ²’æœ‰é‡åˆ°ï¼Œreward ç›¸ç•¶æ–¼æ˜¯ <span>$0$</span> ï¼Œä½†å…¶**å¯èƒ½**æ˜¯å€‹æ¯”å¹³å‡å¥½çš„é¸æ“‡ï¼Œåœ¨åŸå…ˆçš„ policy update å»ä¸æœƒè¢« explore ã€‚\\
ç°¡å–®ä¾†èªªå°±æ˜¯è¦ normlize rewardã€‚

### Causality

Action taken at <span>$t^{\prime} (> t)$</span> cannot affect reward at <span>$t$</span>

å°‡åŸå…ˆçš„ <span>$r(\tau)$</span> å–ä»£æˆ <span>$\sum_{t=t^{\prime}}^T \gamma^{t - t^{\prime}}r_t$</span>

**Note:** <span>$\gamma$</span> ç‚º discount factor (of time)

**Remark:** æŠŠ gradient çš„ weight ç¨±ç‚º **Advantage function** <span>$A^\pi(s_t,a_t)$</span>\\
æ„ç¾©ä¸Šç‚º "How good it is if we take <span>$a_t$</span> than other action at
<span>$s_t$</span>" (ä¹‹å¾Œæœƒæåˆ°çš„ <span>$Q(s,a)$</span>)

## Off-policy

åŸå…ˆ naive çš„ REINFORCE ï¼Œåœ¨å­¸/è¦æ›´æ–°çš„ agent (parametrized by <span>$\theta$</span>) è·Ÿèˆ‡ç’°å¢ƒäº’å‹•çš„ agent/actor æ˜¯åŒä¸€å€‹ï¼Œæˆ‘å€‘ç¨±æ­¤ç‚º **on-policy**ï¼Œä½†é€™æ¨£çš„å£è™•åœ¨æ–¼ trajectory collection è€—æ™‚ï¼Œè€Œä¸”ä¸€æ—¦åƒæ•¸æ›´æ–°äº†ï¼Œåˆè¦å†æ¬¡ resample ï¼Œéå¸¸æ²’æœ‰æ•ˆç‡ã€‚

æ•…å¼•é€² **off-policy** çš„æ–¹æ³•ï¼Œé‡è¤‡ä½¿ç”¨ä¹‹å‰ç²å¾—çš„ trajectoryï¼Œä½†é€™æ¨£çš„ tradeoff åœ¨æ–¼æ‰€è¨ˆç®—çš„ gradient å¯èƒ½ä¸ç²¾ç¢º (ä¸é©åˆç¾åœ¨çš„ agent)ï¼Œæ‰€ä»¥éœ€è¦çš„ constraint å»æ¸›ç·©é€™ä»¶äº‹æƒ…çš„å½±éŸ¿ã€‚

### Importance Sampling

å‡è¨­ç¾åœ¨æˆ‘å€‘æœ‰å…©çµ„ trajectory distribution <span>$p,q$</span>ï¼Œæˆ‘å€‘æƒ³åˆ©ç”¨å¾
<span>$q$</span> æ‰€ sample/collect çš„ trajectory å»æ›´æ–° <span>$p$</span>
(will update coresponding polcy)ï¼Œé‚£éº¼è©²å¦‚ä½•ä¼°æ¸¬ Policy Gradient æ‰€éœ€è¦çš„ expected reward <span>$\mathbb{E}_{\tau \sim p}[r(\tau)]$</span>? é€™æ™‚å€™å°±éœ€è¦ç”¨åˆ° Importance samplingï¼Œæ¨å°å¦‚ä»¥ä¸‹

``$$
\begin{aligned}
\mathbb{E}_{\tau \sim p}[r(\tau)] &= \int_\tau p(\tau) r(\tau) d \tau\\
&= \int_\tau r(\tau) \frac{p(\tau)}{q(\tau)} q(\tau) d\tau \\
&= \mathbb{E}_{\tau \sim q}[r(\tau) \color{blue}{\frac{p(\tau)}{q(\tau)}}]
\end{aligned}
$$``

**Remark:** <span>$\frac{p(\tau)}{q(\tau)}$</span> ç¨±ç‚º importance weight ï¼Œç”¨æ­¤
å» scale ç›¸å°æ‡‰çš„ rewardã€‚

**Remark:** å„˜ç®¡æœŸæœ›å€¼ç›¸åŒï¼Œä½† <span>$\text{Var}\scriptstyle{\tau \sim p}\textstyle[r(\tau)]$</span> å’Œ <span>$\text{Var}\scriptstyle{\tau \sim q} \textstyle[r(\tau) \frac{p(\tau)}{q(\tau)}]$</span>é‚„æ˜¯ä¸åŒçš„ã€‚å…©å€‹ distribution é‚„æ˜¯ä¸èƒ½å·®å¤ªå¤šï¼Œé€™ä¹Ÿæ˜¯ è¦åŠ  constraint çš„æ‰€åœ¨ã€‚

ä¸‹é¢æœ‰ä¸€å¼µç°¡å–®çš„ç¤ºæ„åœ–ï¼ˆä¸é notation æœ‰é»å°ä¸åŒğŸ˜…ï¼‰
<center><img src="/img/post/importance-sampling.png" width="70%" style="border-radius: 0%;"></center>

### Proximal Policy Optimization (PPO)
å°‡åŸå…ˆçš„ objective function <span>$J^{\theta^\prime}(\theta)$</span> åŠ ä¸Šä¸€å€‹è¡¡é‡ <span>$\theta,\theta^\prime$</span> å°æ‡‰ policy çš„ KL divergence\\
(æ‹¿ä¸Šé¢çš„ä¾‹å­ä¾†çœ‹çš„è©±ï¼Œå°±æ˜¯ <span>$\text{KL}(p,q)$</span>)

**Remark:** å¦å¤–æœ‰ä¸€å€‹è®Šå½¢ï¼Œä¸æ˜¯è€ƒæ…® policy çš„ KL divergenceï¼Œè€Œæ˜¯ç”¨ä¸€å€‹ function å» clip <span>$\frac{p(\tau)}{q(\tau)}$</span> å·®å¤ªå¤šçš„æƒ…æ³ã€‚

**Remark:** ä¸éå¯¦å‹™ä¸Š update å¹¾æ¬¡å¾Œï¼Œä¸å¯é¿å…åœ°é‚„æ˜¯è¦é‡æ–° sampleã€‚

## Reference
* [æå®æ¯…è€å¸«çš„æŠ•å½±ç‰‡] (http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/PPO%20(v3).pdf)
