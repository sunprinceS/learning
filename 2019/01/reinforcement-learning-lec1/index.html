<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Policy-based Algorithm">
  <meta name="generator" content="Hugo 0.62.0" />

  <title>Reinforcement Learning - Lec1 &middot; Stay hungry. Stay foolish </title>

    

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://sunprinces.github.io/learning/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://sunprinces.github.io/learning/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="https://sunprinces.github.io/learning/css/blackburn.css">

  
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.9.0/css/all.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  
  
  
  

 
  

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokai.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  
  <script>hljs.initHighlightingOnLoad();</script>
  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  

  <link rel="shortcut icon" href="https://sunprinces.github.io/learning/img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  <style>
  img{
    padding-left: 15px;
    border-radius: 100%
  }
</style>
<a class="pure-menu-heading brand img-circle" target='_blank' href="https://sunprinces.github.io/">
  <img src="https://sunprinces.github.io/learning/img/logo.png" width="145px" >
</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/topics/"><i class='fa fa-folder fa-fw'></i>Topics</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/tags/"><i class='fa fa-tags fa-fw'></i>Tags</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/sunprinceS_0822" rel="me" target="_blank"><i class="fab fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://instagram.com/sunprince822" rel="me" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/sunprinceS" rel="me" target="_blank"><i class="fab fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://sunprincelife.wordpress.com/" target="_blank"><i class="fa fa-coffee fa-fw"></i>Life</a>
    </li>
    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2017. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Reinforcement Learning - Lec1</h1>
  <h2>Policy-based Algorithm</h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>01 Jan 2019</time>
  </div>

  

  
  
  
  <div>
    <i class="fa fa-folder fa-fw"></i>
    
      <a class="post-taxonomy-topic" href="https://sunprinces.github.io/learning/topics/reinforcement-learning">Reinforcement Learning</a>
    
  </div>
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="https://sunprinces.github.io/learning/tags/rl">rl</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://sunprinces.github.io/learning/tags/ntu">NTU</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://sunprinces.github.io/learning/tags/cs294">cs294</a>
    
  </div>
  
  

</div>

  <p>今年1 月的目標想複習三下時學的 RL，主要的參考教材為李宏毅老師的 DRL 8 講及 Sergey 在
Berkeley 開的 CS294。</p>
<p>先讓我們從 Policy Gradient 開始吧！</p>
<h2 id="terminology">Terminology</h2>
<p>這裡假設讀者熟悉 RL 的基本概念 (env/action/state 的交互作用及相關名詞)。</p>
<h3 id="policy">Policy</h3>
<p>Policy <span>$\pi$</span> 為 given state <span>$s$</span>，採取 action <span>$a$</span> 的機率。而我們可以以一組參數 <span>$\theta$</span> 去描述。</p>
<div>
\[
\pi_\theta(a,s) = \mathbb{P}[a|s]
\]
</div>
<p>Given 一個固定的 policy 的 agent (或者又叫做 actor)，我們讓其去與環境互動，每玩一次(一個 episode) 會收集到一個序列 {<span>$s_1,a_1,\cdots,s_T,a_T$</span>}，稱為 trajectory <span>$\tau$</span>，並會得到一個 final reward <span>$r(\tau)$</span></p>
<h3 id="value-function">Value Function</h3>
<p><strong>Given policy</strong>, expected return from state <span>$s$</span></p>
<div>
\[
V^\pi(s) = \mathbb{E}_\pi[r(\tau) | s_t = s]
\]
</div>
<p><strong>Note:</strong> <span>$\pi$</span> 是指 Given policy，特定 trajectory 被 sample 出來的機率</p>
<h3 id="action-value-function">Action-Value Function</h3>
<p><strong>Given policy</strong>, expected return when taking <span>$a$</span> at <span>$s$</span> ，之後稱其為 Q function</p>
<div>
\[
Q^\pi(s,a) = \mathbb{E}_\pi[r(\tau)|s_t = s, a_t = a]
\]
</div>
<h2 id="goal">Goal</h2>
<p>我們想找到一個最好的 policy 去 maximize 最後得到 reward 的期望值 ( optimize 的目標)</p>
<!--### Evaluation Problem (Optimal value function)-->
<h3 id="optimal-value-function">Optimal value function</h3>
<div>
\[
\begin{aligned}
V_{\ast}(s) &= \max_\pi V_\pi(s) = \max_a Q_{\ast}(s,a)\\
Q_{\ast}(s,a) &= \max_\pi Q_\pi(s,a) = r(s,a) + \sum_{s^\prime}
\mathbb{P}[s^\prime|a,s] V_{\ast}(s)
\end{aligned}
\]
</div>
<p><strong>Note: exploration 只在 training 的時候做，一旦摸清整個環境(i.e optimal policy
is claimed)</strong>，在決定時便不用再加上隨機性了 (so that's why I directly write down
<span>$\max$</span>)</p>
<h3 id="decoding-problem-how-to-construct-such-policy">Decoding Problem (how to construct such policy)</h3>
<p>Only considered in non policy-based method, more discussion on <span>$\rightarrow$</span> Lec3</p>
<h3 id="learning-problem">Learning Problem</h3>
<p>而後續討論的演算法，都是圍繞著如何找出這樣符合optimal value equation 的 policy。</p>
<ul>
<li>DP-based <strong>Planning</strong>: value iteration, policy iteration
<ul>
<li>Discrete action/state and <span>$|\mathcal{A}|,|\mathcal{S}|$</span> is small (i.e tabular): \</li>
<li>MDP is known</li>
<li><strong>Contraction Mapping Theorem</strong> 可以 prove 最佳解的存在性) <span>$\rightarrow$</span> see Lec3</li>
</ul>
</li>
<li>Value-based <strong>Learning</strong>: Monte-Carlo RL, Temporal-Difference (bootstrapping) RL
<ul>
<li>In generate, discrete action (since we need to choose argmax from
<span>$\mathcal{A}$</span>)</li>
<li>MDP is unknown</li>
<li>Tradeoff between biased or not and low/high variance</li>
</ul>
</li>
<li>Policy-based <strong>Learning</strong>: REINFORCE
<ul>
<li>Continuous action/state (since we usually use NN as hypothesis set)</li>
<li>MDP is unknown</li>
<li>Variance reduction trick (see <span>$\rightarrow$</span> Lec2)</li>
<li>Combined with value-based algorithm: Actor-Critic (see <span>$\rightarrow$</span> Lec5)</li>
</ul>
</li>
</ul>
<!--## Evaluation Problem: Goodness of <span>$\pi$</span>-->
<!--那現在我們的目標是要找出一組最好的 <span>$\theta^{\ast}$</span>。\\-->
<!--而所謂最好，則是要去 maximize final reward，也就是以下-->
<!--``$$-->
<!--\mathbb{E}_{\tau \; \sim \; \pi_\theta(\tau)} [r(\tau)] = \int_\tau-->
<!--\pi_\theta(\tau) r(\tau) d\tau-->
<!--$$``-->
<h2 id="how-to-update-spanthetaspan-in-policy-gradient">How to update <span>$\theta$</span> in Policy Gradient</h2>
<p>Optimal policy 所對應到的 <span>$\theta$</span> 滿足以下</p>
<div>
\[
\theta^{\ast} \, = \,  \arg \max_\theta \mathbb{E}_{\tau \sim \pi_\theta} r(\tau) \, = \, \arg \max_\theta \sum_t [\mathbb{E}_{(s_t,a_t) \sim \pi_\theta(s,a)} [r(s_t,a_t)]]
\]
</div>
<p>利用 <strong>Graient Descent</strong>。</p>
<p><code>$$ \begin{align} \nabla_\theta J(\theta) &amp;= \int \color{blue}{\nabla_\theta[\pi_\theta(\tau)]} r(\tau) d\tau\\ &amp;= \int \color{blue}{\pi_\theta \nabla_\theta[\log \pi_\theta(\tau)]}r(\tau) d\tau \\ &amp;= \mathbb{E}_{\tau \; \sim \; \pi_\theta(\tau)} \nabla_\theta[\log\pi_\theta(\tau)]r(\tau) \\ \end{align} $$</code></p>
<p>直覺來說，相同於傳統 supervised learning 的 setting ，但更新的步伐大小，跟 reward 成線性關係\<br>
(i.e try &amp; error，哪個方向好就走多一點，weighted by reward)</p>
<p>但上述的 <span>$\nabla_\theta J(\theta)$</span>無法很直覺地用於 policy 的更新，所以我們做了以下改動</p>
<ul>
<li>Policy Gradient Theorem 告訴我們</li>
</ul>
<p>For any differentialble policy <span>$\pi_{\theta}(a,s)$</span>, for any of the policy objective functions <span>$J = J_1,J \scriptstyle avR$</span>&hellip;,\</p>
<div>
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a)Q^{\pi_\theta}(s,a)]
\]
</div>
<ul>
<li>我們是對每一個 <span>$(a,s)$</span> 做更新 (因 <span>$\pi$</span> 能管的也只有 Given <span>$s$</span>，選擇採取 <span>$a$</span> 的機率)</li>
</ul>
<div>
\[
\nabla_\theta [\log \pi_\theta (\tau)] = \sum_t \nabla_\theta [\log \pi_\theta
(a_t , s_t)]
\]
</div>
<ul>
<li>在大多數情況，無法窮舉所有可能的 <span>$\tau$</span> 去求期望值，只能用
sampling 去估計</li>
</ul>
<div>
\[
\boxed{
\nabla_\theta J(\theta) \approx \sum_n \sum_t \nabla_\theta[\log\pi_\theta(a_t,s_t)]\underbrace{\,r(\tau)\,}_{
Q^{\pi_\theta}(s,a)}}
\]
</div>
<p><strong>Note</strong>: 在最基本的 policy gradient 中，我們用 <span>$r(\tau)$</span> 來作為<span>$Q^{\pi_\theta}(s,a)$</span> 的 unbiased estimation。</p>
<h3 id="drawback">Drawback</h3>
<ul>
<li>Online policy，need to resample when <span>$\theta$</span> is updated
<span>$\Rightarrow$</span> Data collection 費時</li>
<li>Sample is finite <span>$\Rightarrow$</span> variance 大，難 train 。</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy Gradient Theorem 的證明</a></li>
</ul>
  
  
<ul class="share-buttons">
	<li><a href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec1%2f" target="_blank" title="Share on Facebook"><i class="fa-facebook" aria-hidden="true"></i><span class="sr-only">Share on Facebook</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://twitter.com/intent/tweet?source=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec1%2f&via=HorribleGeek" target="_blank" title="Tweet"><i class="fa-twitter" aria-hidden="true"></i><span class="sr-only">Tweet</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://plus.google.com/share?url=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec1%2f" target="_blank" title="Share on Google+"><i class="fa-google-plus" aria-hidden="true"></i><span class="sr-only">Share on Google+</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.tumblr.com/share?v=3&u=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec1%2f" target="_blank" title="Post to Tumblr"><i class="fa-tumblr" aria-hidden="true"></i><span class="sr-only">Post to Tumblr</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://pinterest.com/pin/create/button/?url=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec1%2f" target="_blank" title="Pin it"><i class="fa-pinterest-p" aria-hidden="true"></i><span class="sr-only">Pin it</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.reddit.com/submit?url=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec1%2f" target="_blank" title="Submit to Reddit"><i class="fa-reddit-alien" aria-hidden="true"></i><span class="sr-only">Submit to Reddit</span></a>
	</li>
</ul>


<style>
	ul.share-buttons{
	  list-style: none;
	  padding: 0;
	}

	ul.share-buttons li{
	  display: inline;
	}

	ul.share-buttons .sr-only{
	  position: absolute;
	  clip: rect(1px 1px 1px 1px);
	  clip: rect(1px, 1px, 1px, 1px);
	  padding: 0;
	  border: 0;
	  height: 1px;
	  width: 1px;
	  overflow: hidden;
	}
</style>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="https://sunprinces.github.io/learning/2018/11/ntu-machine-learning-lec14/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="https://sunprinces.github.io/learning/2018/11/ntu-machine-learning-lec14/">NTU Machine Learning - Lec14</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
    <nav class="next">
      <a href="https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec2/">Reinforcement Learning - Lec2</a>
    </nav>
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
    <a href="https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec2/"><i class="fa fa-chevron-right"></i></a>
    
  </div>
</div>



  
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'tonyhsu822';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
 <script>renderMathInElement(document.body, {
   delimiters: [
     {left: "\\[", right: "\\]", display: true},
     {left: "$", right: "$", display: false},
   ]
 });</script>



</div>

</div>
</div>
<script src="https://sunprinces.github.io/learning/js/ui.js"></script>
<script src="https://sunprinces.github.io/learning/js/menus.js"></script>


<script>
  
  if (window.location.hostname != "localhost") {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-59748795-3', 'auto');
    ga('send', 'pageview');
  }
</script>





  <script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
           extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  });
  </script>




</body>
</html>

