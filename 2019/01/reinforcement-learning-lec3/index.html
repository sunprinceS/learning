<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Value-Based Algorithm - MC &amp; TD">
  <meta name="generator" content="Hugo 0.62.0" />

  <title>Reinforcement Learning - Lec3 &middot; Stay hungry. Stay foolish </title>

    

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://sunprinces.github.io/learning/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://sunprinces.github.io/learning/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="https://sunprinces.github.io/learning/css/blackburn.css">

  
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.9.0/css/all.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  
  
  
  

 
  

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokai.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  
  <script>hljs.initHighlightingOnLoad();</script>
  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  

  <link rel="shortcut icon" href="https://sunprinces.github.io/learning/img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  <style>
  img{
    padding-left: 15px;
    border-radius: 100%
  }
</style>
<a class="pure-menu-heading brand img-circle" target='_blank' href="https://sunprinces.github.io/">
  <img src="https://sunprinces.github.io/learning/img/logo.png" width="145px" >
</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/topics/"><i class='fa fa-folder fa-fw'></i>Topics</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://sunprinces.github.io/learning/tags/"><i class='fa fa-tags fa-fw'></i>Tags</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/sunprinceS_0822" rel="me" target="_blank"><i class="fab fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://instagram.com/sunprince822" rel="me" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/sunprinceS" rel="me" target="_blank"><i class="fab fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://sunprincelife.wordpress.com/" target="_blank"><i class="fa fa-coffee fa-fw"></i>Life</a>
    </li>
    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2017. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Reinforcement Learning - Lec3</h1>
  <h2>Value-Based Algorithm - MC &amp; TD</h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>05 Jan 2019</time>
  </div>

  

  
  
  
  <div>
    <i class="fa fa-folder fa-fw"></i>
    
      <a class="post-taxonomy-topic" href="https://sunprinces.github.io/learning/topics/reinforcement-learning">Reinforcement Learning</a>
    
  </div>
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="https://sunprinces.github.io/learning/tags/rl">RL</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://sunprinces.github.io/learning/tags/ntu">NTU</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="https://sunprinces.github.io/learning/tags/cs294">CS294</a>
    
  </div>
  
  

</div>

  <p>前兩講 focus 在 policy-based 的 RL 演算法，直接 learn 一組參數去 parametrize
policy。而後續兩講則會 focus 在 value-based 的方法，想法是算出 <!-- raw HTML omitted -->$V^\pi(s), Q^\pi(s,a)$<!-- raw HTML omitted --> (同樣可以 approximately parametrized by <!-- raw HTML omitted -->$\theta$<!-- raw HTML omitted -->)，而所對應的 policy 則是去選擇 Given <!-- raw HTML omitted -->$s$<!-- raw HTML omitted -->，好度最高的 action <!-- raw HTML omitted -->$a$<!-- raw HTML omitted --> (w/ appropriate exploration)。</p>
<h2 id="monte-carlo-learning">Monte-Carlo Learning</h2>
<p>回顧 Lec1，<!-- raw HTML omitted -->$V^\pi(s)$<!-- raw HTML omitted --> 的定義是 expected return from state <!-- raw HTML omitted -->$s$<!-- raw HTML omitted -->， Monte-Carlo 的想法是直接 simulate 一個 trajectory from state <!-- raw HTML omitted -->$s$<!-- raw HTML omitted -->，然後去 reward 是多少， Value function 每一個 episode 更新一次。</p>
<!-- raw HTML omitted -->
<ul>
<li>Variance is high</li>
<li>Unbiased estimate</li>
</ul>
<h2 id="temporal-difference-learning">Temporal Difference Learning</h2>
<p>Use <strong>bootstrapping</strong> to estimate final reward by <!-- raw HTML omitted -->$r(s,a) + V(s \scriptscriptstyle t+1 \textstyle)$<!-- raw HTML omitted -->，每走一步Value
function 都會更新</p>
<!-- raw HTML omitted -->
<ul>
<li>Variance is low</li>
<li>Biased estimate</li>
<li>Efficient (and even suitable for non-episodic game)</li>
</ul>
<p><strong>Remark:</strong></p>
<ul>
<li>
<p>如此的迭代，又稱為 value iteration (如果過程中有 explicitly 表示出 policy ，又叫做 policy iteration)</p>
</li>
<li>
<p>如前所述，這裡的 policy 是根據 <!-- raw HTML omitted -->$V(s \scriptscriptstyle t+1 \textstyle)$<!-- raw HTML omitted --> 或 <!-- raw HTML omitted -->$Q(s,a)$<!-- raw HTML omitted --> 決定 (直接 greedily 選 argmax)</p>
</li>
<li>
<p>在 training 時，會希望增加 對 env. 的 exploration</p>
<ul>
<li><!-- raw HTML omitted -->$\epsilon$<!-- raw HTML omitted -->-greedy: 加上一個<!-- raw HTML omitted -->$\epsilon$<!-- raw HTML omitted --> 的機率選擇其他非 argmax 的 action</li>
<li>Boltzmann exploration: follow <!-- raw HTML omitted -->$\exp(Q(s,a))$<!-- raw HTML omitted -->，並可用 temperature <!-- raw HTML omitted -->$T$<!-- raw HTML omitted --> (a hyperparameter) 調整 exploration/exploition 的比重</li>
</ul>
</li>
<li>
<p>用 <!-- raw HTML omitted -->$V$<!-- raw HTML omitted --> 做 criteria 的話，需要先知道 MDP 的 transition (但通常我們不知道)，\<br>
所以一般來說是利用 <!-- raw HTML omitted -->$Q^\pi(s,a)$<!-- raw HTML omitted --> 當成 criteria，也是眾所周知的 <strong>Q-learning</strong> (Q-value iteration)</p>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>一般在 <!-- raw HTML omitted -->$|\mathcal{S}|$<!-- raw HTML omitted -->, <!-- raw HTML omitted -->$|\mathcal{A}|$<!-- raw HTML omitted --> 不小的情況下，會用 function family (parameterized by <!-- raw HTML omitted -->$\phi$<!-- raw HTML omitted -->) 去表示 <!-- raw HTML omitted -->$Q(s,a)$<!-- raw HTML omitted -->，而如何找到 <!-- raw HTML omitted -->$\phi$<!-- raw HTML omitted --> 則是一個 regression problem</li>
</ul>
<!-- raw HTML omitted -->
<p><strong>Note:</strong> <!-- raw HTML omitted -->$\alpha$<!-- raw HTML omitted --> 表示 value 隨新 data 更新的敏感度</p>
<h2 id="convergence-to-optimal-policy-tabular-case">Convergence to optimal policy (tabular case)</h2>
<p>條件:</p>
<ul>
<li>MDP is known</li>
<li><!-- raw HTML omitted -->$|\mathcal{S}|, |\mathcal{A}|$<!-- raw HTML omitted --> is small</li>
</ul>
<p><strong>Remark</strong></p>
<p>所對應的 optimal value function 為下列 equation (先不管 <!-- raw HTML omitted -->$\alpha$<!-- raw HTML omitted -->)</p>
<!-- raw HTML omitted -->
<ul>
<li>
<p>由上方 equation 可知 <!-- raw HTML omitted -->$Q_\ast$<!-- raw HTML omitted -->  是上述 TD learning iteration operator <!-- raw HTML omitted -->$\mathcal{B}$<!-- raw HTML omitted --> 的 fixed point (前提是知道 <!-- raw HTML omitted -->$\mathbb{P}[s^\prime|s,a]$<!-- raw HTML omitted -->)，<!-- raw HTML omitted -->$Q_\ast = \mathcal{B} Q_\ast$<!-- raw HTML omitted --></p>
</li>
<li>
<p><!-- raw HTML omitted -->$\mathcal{B}$<!-- raw HTML omitted --> is a <em>contraction</em>, <!-- raw HTML omitted -->$||\mathcal{B}Q -
\mathcal{B} \bar{Q}||_{\infty} \leq \gamma ||Q - \bar{Q}||_{\infty}$<!-- raw HTML omitted --></p>
</li>
</ul>
<p>由 <strong>Contraction Mapping Theorem</strong> 可知，<!-- raw HTML omitted -->$Q(s,a)$<!-- raw HTML omitted --> 會以<!-- raw HTML omitted -->$\gamma$<!-- raw HTML omitted --> 的 rate converge 到 <!-- raw HTML omitted -->$Q_\ast$<!-- raw HTML omitted --></p>
<h2 id="non-convergence-to-optimal-policy-approx-spanq-phisaspan-case">Non-convergence to optimal policy (approx. <!-- raw HTML omitted -->$Q_\phi(s,a)$<!-- raw HTML omitted --> case)</h2>
<p>現在我們把一個條件放寬，假設 <!-- raw HTML omitted -->$|\mathcal{S}|$<!-- raw HTML omitted -->, <!-- raw HTML omitted -->$|\mathcal{A}|$<!-- raw HTML omitted --> 大到無法以 table 儲存，只能以一個 function family 去表示 <!-- raw HTML omitted -->$Q(s,a)$<!-- raw HTML omitted --></p>
<p>在做 regression 找當前 iteration 最好的 <!-- raw HTML omitted -->$\phi$<!-- raw HTML omitted --> 時，其實引進了一個 projection operator <!-- raw HTML omitted -->$\Pi$<!-- raw HTML omitted --><br>
(project on function family, <strong>with l2-norm</strong>)</p>
<p>可以證明 <!-- raw HTML omitted -->$\Pi$<!-- raw HTML omitted --> 也是一個 <em>contraction</em> (w.r.t l2-norm)，但
<!-- raw HTML omitted -->$\mathcal{B}$<!-- raw HTML omitted --> 卻是 contraction w.r.t <!-- raw HTML omitted -->$\infty$<!-- raw HTML omitted -->-norm<br>
所以 <!-- raw HTML omitted -->$\mathcal{B}\Pi$<!-- raw HTML omitted --> 並不是 contraction，也就不保證會收斂。</p>
<p><strong>Remark:</strong> 有人可能會想說，那這樣不如就把 regression 改成 w.r.t <!-- raw HTML omitted -->$\infty$<!-- raw HTML omitted -->-norm  的 optimization 問題，但這樣的 optimization 是 intractable 的 (belong to NPC)</p>
  
  
<ul class="share-buttons">
	<li><a href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec3%2f" target="_blank" title="Share on Facebook"><i class="fa-facebook" aria-hidden="true"></i><span class="sr-only">Share on Facebook</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://twitter.com/intent/tweet?source=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec3%2f&via=HorribleGeek" target="_blank" title="Tweet"><i class="fa-twitter" aria-hidden="true"></i><span class="sr-only">Tweet</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="https://plus.google.com/share?url=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec3%2f" target="_blank" title="Share on Google+"><i class="fa-google-plus" aria-hidden="true"></i><span class="sr-only">Share on Google+</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.tumblr.com/share?v=3&u=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec3%2f" target="_blank" title="Post to Tumblr"><i class="fa-tumblr" aria-hidden="true"></i><span class="sr-only">Post to Tumblr</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://pinterest.com/pin/create/button/?url=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec3%2f" target="_blank" title="Pin it"><i class="fa-pinterest-p" aria-hidden="true"></i><span class="sr-only">Pin it</span></a>
	</li>&nbsp;&nbsp;&nbsp;
	<li><a href="http://www.reddit.com/submit?url=https%3a%2f%2fsunprinces.github.io%2flearning%2f2019%2f01%2freinforcement-learning-lec3%2f" target="_blank" title="Submit to Reddit"><i class="fa-reddit-alien" aria-hidden="true"></i><span class="sr-only">Submit to Reddit</span></a>
	</li>
</ul>


<style>
	ul.share-buttons{
	  list-style: none;
	  padding: 0;
	}

	ul.share-buttons li{
	  display: inline;
	}

	ul.share-buttons .sr-only{
	  position: absolute;
	  clip: rect(1px 1px 1px 1px);
	  clip: rect(1px, 1px, 1px, 1px);
	  padding: 0;
	  border: 0;
	  height: 1px;
	  width: 1px;
	  overflow: hidden;
	}
</style>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec2/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec2/">Reinforcement Learning - Lec2</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
    <nav class="next">
      <a href="https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec4/">Reinforcement Learning - Lec4</a>
    </nav>
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
    <a href="https://sunprinces.github.io/learning/2019/01/reinforcement-learning-lec4/"><i class="fa fa-chevron-right"></i></a>
    
  </div>
</div>



  
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'tonyhsu822';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
 <script>renderMathInElement(document.body, {
   delimiters: [
     {left: "\\[", right: "\\]", display: true},
     {left: "$", right: "$", display: false},
   ]
 });</script>



</div>

</div>
</div>
<script src="https://sunprinces.github.io/learning/js/ui.js"></script>
<script src="https://sunprinces.github.io/learning/js/menus.js"></script>


<script>
  
  if (window.location.hostname != "localhost") {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-59748795-3', 'auto');
    ga('send', 'pageview');
  }
</script>





  <script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
           extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  });
  </script>




</body>
</html>

